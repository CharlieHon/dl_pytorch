{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 创建Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7.3988e+31, 4.4849e+21, 2.7370e+20],\n",
       "        [6.4640e-04, 1.9823e+29, 1.8810e+31],\n",
       "        [4.4339e+27, 1.7975e+19, 6.9481e+22],\n",
       "        [1.7743e+28, 2.0535e-19, 1.4226e-13],\n",
       "        [1.8037e+28, 7.2296e+31, 3.2605e-12]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.empty(5, 3)   # 创建一个5×3的未初始化的`Tensor`\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3951, 0.6583, 0.4607],\n",
       "        [0.8427, 0.5753, 0.3884],\n",
       "        [0.6411, 0.9887, 0.7268],\n",
       "        [0.6892, 0.2425, 0.5747],\n",
       "        [0.3250, 0.2380, 0.7643]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(5, 3)    # 创建一个5×3的随机初始化的`Tensor`\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros(5, 3, dtype=torch.long) # 创建一个5×3的long型全0的`Tensor`\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.0000, 5.3000])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([5, 5.3])  # 直接根据数据创建\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "tensor([[-0.9781,  1.7032,  1.4038],\n",
      "        [-2.2837,  0.6084,  2.4455],\n",
      "        [ 1.6116, -0.4963, -1.2148],\n",
      "        [-0.7958, -1.6141,  0.4639],\n",
      "        [ 0.3047, -0.8630,  0.5120]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# 根据现有的`Tensor`创建，默认重用输入`Tensor`的一些属性，例如数据类型\n",
    "x = x.new_ones(5, 3, dtype=torch.float64)   # 返回的x默认具有相同的torch.dtype\n",
    "print(x)\n",
    "\n",
    "x = torch.randn_like(x)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n",
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "# 通过`shape`或者`size()`来获取`Tensor`的形状\n",
    "print(x.size())\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]]) tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n",
      "tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.]]) tensor([1, 3, 5, 7, 9])\n",
      "\n",
      "tensor([ 0.0000,  0.1010,  0.2020,  0.3030,  0.4040,  0.5051,  0.6061,  0.7071,\n",
      "         0.8081,  0.9091,  1.0101,  1.1111,  1.2121,  1.3131,  1.4141,  1.5152,\n",
      "         1.6162,  1.7172,  1.8182,  1.9192,  2.0202,  2.1212,  2.2222,  2.3232,\n",
      "         2.4242,  2.5253,  2.6263,  2.7273,  2.8283,  2.9293,  3.0303,  3.1313,\n",
      "         3.2323,  3.3333,  3.4343,  3.5354,  3.6364,  3.7374,  3.8384,  3.9394,\n",
      "         4.0404,  4.1414,  4.2424,  4.3434,  4.4444,  4.5455,  4.6465,  4.7475,\n",
      "         4.8485,  4.9495,  5.0505,  5.1515,  5.2525,  5.3535,  5.4545,  5.5556,\n",
      "         5.6566,  5.7576,  5.8586,  5.9596,  6.0606,  6.1616,  6.2626,  6.3636,\n",
      "         6.4646,  6.5657,  6.6667,  6.7677,  6.8687,  6.9697,  7.0707,  7.1717,\n",
      "         7.2727,  7.3737,  7.4747,  7.5758,  7.6768,  7.7778,  7.8788,  7.9798,\n",
      "         8.0808,  8.1818,  8.2828,  8.3838,  8.4848,  8.5859,  8.6869,  8.7879,\n",
      "         8.8889,  8.9899,  9.0909,  9.1919,  9.2929,  9.3939,  9.4950,  9.5960,\n",
      "         9.6970,  9.7980,  9.8990, 10.0000]) tensor([[0.1436, 0.4282],\n",
      "        [0.8863, 0.9554]])\n",
      "\n",
      "tensor([[-0.2664,  0.0782],\n",
      "        [-0.3669,  2.1485]]) tensor([[-0.2198, -0.1277],\n",
      "        [-0.3112,  0.2567]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 创建全1张量\n",
    "a = torch.ones(2, 2)\n",
    "# 创建全0张量\n",
    "b = torch.zeros(2, 3)\n",
    "# 创建对角线为1，其他为0的张量\n",
    "c = torch.eye(3, 4)\n",
    "# 创建指定范围和步长的向量\n",
    "d = torch.arange(1, 10, 2)\n",
    "# 创建从开始到结束均匀分n份向量\n",
    "e = torch.linspace(0, 10, 100)\n",
    "# 均匀/标准分布\n",
    "f = torch.rand(2, 2)\n",
    "g = torch.randn(2, 2)\n",
    "# 正态分布/均匀分布\n",
    "h = torch.normal(0, 1, (2, 2))\n",
    "print(a, b, end='\\n\\n')\n",
    "print(c, d, end='\\n\\n')\n",
    "print(e, f, end='\\n\\n')\n",
    "print(g, h, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1505,  1.9058,  0.9361],\n",
       "        [-0.7356,  1.0195,  0.0900],\n",
       "        [-0.4226, -1.3690, -1.0433],\n",
       "        [ 0.2169, -1.1246, -0.2390],\n",
       "        [-0.5515, -1.0224, -0.6020]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "y = torch.randn(5, 3)\n",
    "x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1505,  1.9058,  0.9361],\n",
       "        [-0.7356,  1.0195,  0.0900],\n",
       "        [-0.4226, -1.3690, -1.0433],\n",
       "        [ 0.2169, -1.1246, -0.2390],\n",
       "        [-0.5515, -1.0224, -0.6020]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.add(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1505,  1.9058,  0.9361],\n",
       "        [-0.7356,  1.0195,  0.0900],\n",
       "        [-0.4226, -1.3690, -1.0433],\n",
       "        [ 0.2169, -1.1246, -0.2390],\n",
       "        [-0.5515, -1.0224, -0.6020]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 就地操作，在原来y的基础上加x，add x to y\n",
    "y.add_(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `PyTorch`操作inplace版本都有后缀`_`，例如`x.copy_(y)`, `x.t_()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 索引\n",
    "\n",
    "索引出来的结果与原数据共享内存，也即修改一个，另一个会跟着修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4])\n",
      "tensor([[ 1,  2,  3,  4],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(12).reshape(3, 4)\n",
    "y = x[0]\n",
    "y += 1  # 等价于：y.add_(1)\n",
    "print(y)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  2,  3,  4],\n",
      "        [ 8,  9, 10, 11]])\n",
      "tensor([[ 1,  3],\n",
      "        [ 4,  6],\n",
      "        [ 8, 10]])\n"
     ]
    }
   ],
   "source": [
    "# 其他索引方式\n",
    "indices = torch.tensor([0, 2])\n",
    "print(torch.index_select(x, 0, indices))\n",
    "print(torch.index_select(x, 1, indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 改变形状\n",
    "\n",
    "  - `view()`返回新的`Tensor`与原`Tensor`虽然可能有不同的`size`，但是共享`data`。\n",
    "  - `reshape()`也可以改变形状，但是此函数不能保证返回的是拷贝还是共享(数据内存连续时)\n",
    "  - 可以先用`clone`创造一个副本然后再使用`view`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1,  0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12, 13]])\n",
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(15).reshape(3, 5)\n",
    "x_cp = x.clone().view(15)\n",
    "x -= 1\n",
    "print(x)\n",
    "print(x_cp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 使用`clone`时还有一个好处是会被记录在计算图中，即梯度回传到副本时也会被传到源`Tensor`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外一个常用的函数就是`item()`，可以将一个标量`Tensor`转换成一个Python number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.4541])\n",
      "-1.4540530443191528\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1)\n",
    "print(x)\n",
    "print(x.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线性代数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 0, 9, 3],\n",
      "        [2, 2, 8, 9],\n",
      "        [2, 5, 9, 6],\n",
      "        [9, 8, 7, 8]])\n",
      "tensor(20)\n",
      "tensor([1, 2, 9, 8])\n",
      "tensor([[1, 0, 9, 3],\n",
      "        [0, 2, 8, 9],\n",
      "        [0, 0, 9, 6],\n",
      "        [0, 0, 0, 8]])\n",
      "tensor([[1, 2, 2, 9],\n",
      "        [0, 2, 5, 8],\n",
      "        [9, 8, 9, 7],\n",
      "        [3, 9, 6, 8]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randint(10, (4, 4))\n",
    "print(x)\n",
    "# 对角元素之和\n",
    "print(torch.trace(x))\n",
    "# 对角线元素\n",
    "print(torch.diag(x))\n",
    "# 矩阵的上三角/下三角，可指定偏移量\n",
    "print(torch.triu(x))\n",
    "# 转置\n",
    "print(x.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 广播机制\n",
    "\n",
    "对两个形状不同的`Tensor`按元素运算时，可能会触发广播(broadcasting)机制：先适当复制元素使这两个`Tensor`形状相同后再按元素运算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "tensor([[2, 3],\n",
      "        [3, 4],\n",
      "        [4, 5]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(1, 3).view(2,)\n",
    "print(x)\n",
    "y = torch.arange(1, 4).view(3, 1)\n",
    "print(y)\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4 运算的内存开销\n",
    "\n",
    "索引不会开辟新内存，而像`y = x + y`会开新内存，然后`y`指向新内存。\n",
    "\n",
    "python自带的`id`函数，输出变量内存地址。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2])\n",
    "y = torch.tensor([3, 4])\n",
    "id_before = id(y)\n",
    "y = y + x\n",
    "print(id(y) == id_before)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果指定结果到原来的`y`的内存，可以使用前面的索引来进行替换操作。如下，把`x + y`的结果通过`[:]`写进`y`对应的内存中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2])\n",
    "y = torch.tensor([3, 4])\n",
    "id_before = id(y)\n",
    "y[:] = y + x\n",
    "print(id(y) == id_before)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还可以使用运算符全名函数中的`out`参数或者自加运算符`+=`(也即`add_()`)达到上述效果，例如`torch.add(x, y, oy=ut=y)`和`y += x`(`y.add_(x)`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2])\n",
    "y = torch.tensor([3, 4])\n",
    "id_before = id(y)\n",
    "torch.add(x, y, out=y)  # y += x, y.add_(x)\n",
    "print(id(y) == id_before)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 虽然`view()`返回的`Tensor`与源`Tensor`是共享`data`的，但是依然是一个新的`Tensor`(因为`Tensor`除了包含`data`外还包含一些其他属性)，两者`id(内存地址)`并不一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2])\n",
    "y = torch.tensor([3, 4])\n",
    "z = y.view(2)\n",
    "id(y) == id(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.25 `Tensor`和`Numpy`互相转换\n",
    "\n",
    "很容易用`numpy()`和`from_numpy()`将`Tensor`和`Numpy`中的数据互相转换。需要注意的是：这两个函数所产生的`Tensor`和`Numpy()`中的数组共享相同的内存(所以他们之间转换很快)，改变其中一个时另一个会改变！\n",
    "\n",
    "> 使用`torch.tensor()`转换时，进行数据拷贝，返回的`Tensor`和原来数据不再共享内存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.]) [1. 1. 1. 1. 1.]\n",
      "tensor([2., 2., 2., 2., 2.]) [2. 2. 2. 2. 2.]\n",
      "tensor([3., 3., 3., 3., 3.]) [3. 3. 3. 3. 3.]\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "b = a.numpy()\n",
    "print(a, b)\n",
    "\n",
    "a += 1\n",
    "print(a, b)\n",
    "\n",
    "b += 1\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.] tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "[2. 2. 2. 2. 2.] tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
      "[3. 3. 3. 3. 3.] tensor([3., 3., 3., 3., 3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "print(a, b)\n",
    "\n",
    "a += 1\n",
    "print(a, b)\n",
    "\n",
    "b += 1\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所有在CPU上`Tensor`(除了`CharTensor`)都支持与Numpy数组相互转换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4. 4. 4. 4. 4.] tensor([3., 3., 3., 3., 3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# torch.Tensor\n",
    "c = torch.tensor(a)\n",
    "\n",
    "a += 1\n",
    "print(a, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.6 `Tensor` on GPU\n",
    "\n",
    "用方法`to()`可以将`Tensor`在CPU和GPU(需要硬件支持)支持相互移动"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5,  4,  3,  1],\n",
      "        [ 1,  7,  5,  3],\n",
      "        [ 3, 10,  5,  9]], device='cuda:0')\n",
      "tensor([[ 5.,  4.,  3.,  1.],\n",
      "        [ 1.,  7.,  5.,  3.],\n",
      "        [ 3., 10.,  5.,  9.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# 以下代码只有在`Pytorch GPU`版本上才会执行\n",
    "x = torch.randint(10, (3, 4))\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    y = torch.ones_like(x, device=device)\n",
    "    x = x.to(device)\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('pytorch1.12')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad951275103bbfb3096341f5c4674f3a26bc135378116ec035f34c58e555fae9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
